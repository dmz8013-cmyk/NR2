import asyncio
from telegram import Bot
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import pytz

BOT_TOKEN = "8591331989:AAEO0MuLnyFypcslPHMo8mWjW3LNy9BwejM"
CHAT_ID = "@gazzzza2025"
sent_news = set()

KST = pytz.timezone('Asia/Seoul')

# í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (dada_news2 ë°©ì‹)
KEYWORDS = [
    "ë‹¨ë…", "ì†ë³´", "êµ¬ê¸€", "ì•„ë§ˆì¡´", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "ë¨¸ìŠ¤í¬", "í…ŒìŠ¬ë¼", 
    "ì• í”Œ", "ê³¨ë“œë§Œ", "ìµœì´ˆ", "ìƒì¥", "ë‚˜ìŠ¤ë‹¥", "ê¸°ìˆ ì´ì „", "ì¸ìˆ˜", "ëŒ€ë€",
    "AI", "ì¸ê³µì§€ëŠ¥", "ì±—GPT", "ì‚¼ì„±", "SK", "LG", "í˜„ëŒ€", "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤",
    "ì—¬ë¡ ì¡°ì‚¬", "ê¸°íš", "ëŒ€ì„ ", "ì´ì„ ", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì£¼ê°€", "ì¦ì‹œ"
]

async def send_news(bot, news_type, title, link, source=""):
    try:
        emoji_map = {
            "ì†ë³´": "ğŸ””",
            "ë‹¨ë…": "ğŸ¯",
            "ê¸°íš": "ğŸ“‹",
            "ì—¬ë¡ ì¡°ì‚¬": "ğŸ“Š",
            "í‚¤ì›Œë“œ": "ğŸ“°"
        }
        emoji = emoji_map.get(news_type, "ğŸ“°")
        
        source_text = f"ğŸ“° {source}\n" if source else ""
        message = f"{emoji} <b>{news_type}</b>\n\n{title}\n\n{source_text}ğŸ”— {link}"
        await bot.send_message(CHAT_ID, message, parse_mode="HTML")
        print(f"âœ… [{news_type}] {title[:40]}... ({source})")
        return True
    except Exception as e:
        print(f"âŒ {e}")
        return False

def check_keywords(title):
    """í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    title_lower = title.lower()
    for keyword in KEYWORDS:
        if keyword.lower() in title_lower:
            return True
    return False

async def fetch_naver_sections(bot):
    """ë„¤ì´ë²„ ì„¹ì…˜ë³„ ìµœì‹  ê¸°ì‚¬"""
    sections = [
        ("100", "ì •ì¹˜"),
        ("101", "ê²½ì œ"),
        ("102", "ì‚¬íšŒ"),
        ("103", "ìƒí™œë¬¸í™”"),
        ("104", "êµ­ì œ"),
        ("105", "ITê³¼í•™"),
    ]
    
    for sid, name in sections:
        try:
            url = f"https://news.naver.com/section/{sid}"
            r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
            soup = BeautifulSoup(r.text, "html.parser")
            
            articles = soup.select("div.sa_text a.sa_text_title")[:15]
            
            for article in articles:
                title = article.get_text(strip=True)
                link = article.get("href", "")
                
                if not link or link in sent_news:
                    continue
                
                # [ì†ë³´] ìš°ì„ 
                if "[ì†ë³´]" in title:
                    await send_news(bot, "ì†ë³´", title, link, f"ë„¤ì´ë²„{name}")
                    sent_news.add(link)
                    await asyncio.sleep(1)
                
                # [ë‹¨ë…] / (ë‹¨ë…)
                elif "[ë‹¨ë…]" in title or "(ë‹¨ë…)" in title:
                    await send_news(bot, "ë‹¨ë…", title, link, f"ë„¤ì´ë²„{name}")
                    sent_news.add(link)
                    await asyncio.sleep(1)
                
                # [ê¸°íš] / (ê¸°íš)
                elif "[ê¸°íš]" in title or "(ê¸°íš)" in title:
                    await send_news(bot, "ê¸°íš", title, link, f"ë„¤ì´ë²„{name}")
                    sent_news.add(link)
                    await asyncio.sleep(1)
                
                # ì—¬ë¡ ì¡°ì‚¬
                elif "ì—¬ë¡ ì¡°ì‚¬" in title:
                    await send_news(bot, "ì—¬ë¡ ì¡°ì‚¬", title, link, f"ë„¤ì´ë²„{name}")
                    sent_news.add(link)
                    await asyncio.sleep(1)
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ (ì •ì¹˜/ê²½ì œ/ITë§Œ)
                elif sid in ["100", "101", "105"] and check_keywords(title):
                    await send_news(bot, "í‚¤ì›Œë“œ", title, link, f"ë„¤ì´ë²„{name}")
                    sent_news.add(link)
                    await asyncio.sleep(1)
                    
        except Exception as e:
            print(f"âŒ {name}: {e}")

async def fetch_naver_main(bot):
    """ë„¤ì´ë²„ ë©”ì¸ í—¤ë“œë¼ì¸"""
    try:
        url = "https://news.naver.com/"
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        
        headlines = soup.select("a.cjs_news_tw, a.cjs_t")[:20]
        
        for headline in headlines:
            title = headline.get_text(strip=True)
            link = headline.get("href", "")
            
            if not link or link in sent_news:
                continue
            
            if not link.startswith("http"):
                link = "https://news.naver.com" + link
            
            # [ì†ë³´]ë§Œ ë©”ì¸ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if "[ì†ë³´]" in title:
                await send_news(bot, "ì†ë³´", title, link, "ë„¤ì´ë²„ë©”ì¸")
                sent_news.add(link)
                await asyncio.sleep(1)
                
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ë©”ì¸: {e}")

async def main():
    print("ğŸ¤– ëˆ„ë ë´‡ ìµœì¢…íŒ!")
    print("ğŸ“° ë„¤ì´ë²„ ì„¹ì…˜ + í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§")
    print("ğŸ”” [ì†ë³´] + ğŸ¯ [ë‹¨ë…] + ğŸ“‹ [ê¸°íš] + ğŸ“Š ì—¬ë¡ ì¡°ì‚¬ + ğŸ“° í‚¤ì›Œë“œ")
    print("ğŸ“¢ ì±„ë„: @gazzzza2025")
    print("â° 1ë¶„ë§ˆë‹¤ í™•ì¸\n")
    
    bot = Bot(BOT_TOKEN)
    
    try:
        now = datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')
        await bot.send_message(
            CHAT_ID, 
            f"ğŸ¤– <b>ëˆ„ë ë´‡ ìµœì¢…íŒ</b>\n\n"
            f"â° {now}\n"
            f"ğŸ“° ë„¤ì´ë²„ ì„¹ì…˜ + í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§\n"
            f"âœ… ì •ì¹˜/ê²½ì œ/IT ì¤‘ì‹¬ ì‹¤ì‹œê°„ ë‰´ìŠ¤",
            parse_mode="HTML"
        )
    except:
        pass
    
    while True:
        print(f"â° {datetime.now(KST).strftime('%H:%M:%S')} - ë‰´ìŠ¤ í™•ì¸ ì¤‘...")
        
        # ë„¤ì´ë²„ ë©”ì¸ (ì†ë³´ë§Œ)
        await fetch_naver_main(bot)
        
        # ë„¤ì´ë²„ ì„¹ì…˜ë³„ (ì†ë³´/ë‹¨ë…/ê¸°íš/ì—¬ë¡ ì¡°ì‚¬/í‚¤ì›Œë“œ)
        await fetch_naver_sections(bot)
        
        print("â³ 1ë¶„ ëŒ€ê¸°...\n")
        await asyncio.sleep(30)

asyncio.run(main())
