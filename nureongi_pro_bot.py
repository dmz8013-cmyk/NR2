import asyncio
from telegram import Bot
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import pytz
import json
import os
import feedparser
import hashlib
from collections import defaultdict

BOT_TOKEN = "8591331989:AAEO0MuLnyFypcslPHMo8mWjW3LNy9BwejM"
CHAT_ID = "@gazzzza2025"

# ì €ì¥ íŒŒì¼
SENT_NEWS_FILE = "sent_news.json"
KEYWORD_TRACKER_FILE = "keyword_tracker.json"

KST = pytz.timezone('Asia/Seoul')

# ğŸ”¥ ì£¼ìš” ì–¸ë¡ ì‚¬ RSS í”¼ë“œ
RSS_FEEDS = {
    "ì—°í•©ë‰´ìŠ¤": [
        ("https://www.yonhapnewstv.co.kr/category/news/politics/feed/", "ì •ì¹˜"),
        ("https://www.yonhapnewstv.co.kr/category/news/economy/feed/", "ê²½ì œ"),
        ("https://www.yonhapnewstv.co.kr/category/news/society/feed/", "ì‚¬íšŒ"),
    ],
    "ë‰´ì‹œìŠ¤": "https://www.newsis.com/RSS/sokbo.xml",
    "í•œêµ­ê²½ì œ": [
        ("https://www.hankyung.com/feed/politics", "ì •ì¹˜"),
        ("https://www.hankyung.com/feed/economy", "ê²½ì œ"),
    ],
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/30100041/",
    "ì„œìš¸ê²½ì œ": "https://www.sedaily.com/RSS/S11.xml",
    "í•œê²¨ë ˆ": "https://www.hani.co.kr/rss/",
}

# ì¤‘ë³µ ì¶”ì 
sent_news = set()
title_hashes = set()
keyword_last_sent = defaultdict(lambda: datetime.min)

def load_data():
    """ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
    global sent_news, title_hashes
    
    if os.path.exists(SENT_NEWS_FILE):
        with open(SENT_NEWS_FILE, 'r') as f:
            data = json.load(f)
            sent_news = set(data.get('urls', []))
            title_hashes = set(data.get('hashes', []))

def save_data():
    """ë°ì´í„° ì˜êµ¬ ì €ì¥"""
    with open(SENT_NEWS_FILE, 'w') as f:
        json.dump({
            'urls': list(sent_news),
            'hashes': list(title_hashes)
        }, f)

def get_title_hash(title):
    """ì œëª© í•´ì‹œê°’ ìƒì„±"""
    clean_title = title.replace("[ì†ë³´]", "").replace("[ë‹¨ë…]", "").replace("(ë‹¨ë…)", "").strip()
    return hashlib.md5(clean_title.encode()).hexdigest()

def is_recent(pub_date, minutes=10):
    """ìµœê·¼ Në¶„ ì´ë‚´ ê¸°ì‚¬ì¸ì§€ í™•ì¸"""
    try:
        if not pub_date:
            return True
        
        now = datetime.now(KST)
        article_time = datetime(*pub_date[:6], tzinfo=KST)
        
        return (now - article_time).total_seconds() < (minutes * 60)
    except:
        return True

def extract_keywords(title):
    """ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = ["AI", "ì±—GPT", "ì‚¼ì„±", "SK", "LG", "í˜„ëŒ€", "í…ŒìŠ¬ë¼", "ë¨¸ìŠ¤í¬", "ì• í”Œ", 
                "êµ¬ê¸€", "ì•„ë§ˆì¡´", "ê¸ˆë¦¬", "í™˜ìœ¨", "ëŒ€ì„ ", "ì´ì„ ", "ì—¬ë¡ ì¡°ì‚¬"]
    
    for keyword in keywords:
        if keyword in title:
            return keyword
    return None

def should_send_keyword_news(title):
    """í‚¤ì›Œë“œ ì¤‘ë³µ ì²´í¬ (5ë¶„ ì´ë‚´ ë™ì¼ í‚¤ì›Œë“œëŠ” 1ê°œë§Œ)"""
    keyword = extract_keywords(title)
    if not keyword:
        return True
    
    now = datetime.now(KST)
    last_sent = keyword_last_sent[keyword]
    
    if (now - last_sent).total_seconds() > 300:  # 5ë¶„
        keyword_last_sent[keyword] = now
        return True
    
    return False

async def send_news(bot, news_type, title, link, source=""):
    """ë‰´ìŠ¤ ì „ì†¡"""
    try:
        emoji_map = {
            "ì†ë³´": "ğŸ””",
            "ë‹¨ë…": "ğŸ¯",
            "ê¸°íš": "ğŸ“‹",
            "ì—¬ë¡ ì¡°ì‚¬": "ğŸ“Š"
        }
        emoji = emoji_map.get(news_type, "ğŸ“°")
        
        source_text = f"ğŸ“° {source}\n" if source else ""
        message = f"{emoji} <b>{news_type}</b>\n\n{title}\n\n{source_text}ğŸ”— {link}"
        
        await bot.send_message(CHAT_ID, message, parse_mode="HTML")
        print(f"âœ… [{news_type}] {title[:40]}... ({source})")
        return True
    except Exception as e:
        print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False

async def fetch_rss_feed(bot, feed_url, source_name, category=""):
    """RSS í”¼ë“œ í¬ë¡¤ë§"""
    try:
        feed = feedparser.parse(feed_url)
        source = f"{source_name}{category}"
        
        for entry in feed.entries[:20]:
            title = entry.get('title', '')
            link = entry.get('link', '')
            pub_date = entry.get('published_parsed')
            
            if not title or not link:
                continue
            
            # ì¤‘ë³µ ì²´í¬
            title_hash = get_title_hash(title)
            if link in sent_news or title_hash in title_hashes:
                continue
            
            # ìµœê·¼ ê¸°ì‚¬ë§Œ (10ë¶„ ì´ë‚´)
            if not is_recent(pub_date, minutes=10):
                continue
            
            # [ì†ë³´] ìµœìš°ì„ 
            if "[ì†ë³´]" in title or "ì†ë³´" in title:
                await send_news(bot, "ì†ë³´", title, link, source)
                sent_news.add(link)
                title_hashes.add(title_hash)
                save_data()
                await asyncio.sleep(1)
            
            # [ë‹¨ë…]
            elif "[ë‹¨ë…]" in title or "(ë‹¨ë…)" in title:
                await send_news(bot, "ë‹¨ë…", title, link, source)
                sent_news.add(link)
                title_hashes.add(title_hash)
                save_data()
                await asyncio.sleep(1)
            
            # [ê¸°íš]
            elif "[ê¸°íš]" in title or "(ê¸°íš)" in title:
                await send_news(bot, "ê¸°íš", title, link, source)
                sent_news.add(link)
                title_hashes.add(title_hash)
                save_data()
                await asyncio.sleep(1)
            
            # ì—¬ë¡ ì¡°ì‚¬
            elif "ì—¬ë¡ ì¡°ì‚¬" in title:
                await send_news(bot, "ì—¬ë¡ ì¡°ì‚¬", title, link, source)
                sent_news.add(link)
                title_hashes.add(title_hash)
                save_data()
                await asyncio.sleep(1)
                
    except Exception as e:
        print(f"âŒ RSS ì˜¤ë¥˜ ({source_name}): {e}")

async def fetch_all_rss(bot):
    """ëª¨ë“  RSS í”¼ë“œ ë³‘ë ¬ ì²˜ë¦¬"""
    tasks = []
    
    # ì—°í•©ë‰´ìŠ¤
    for feed_url, category in RSS_FEEDS["ì—°í•©ë‰´ìŠ¤"]:
        tasks.append(fetch_rss_feed(bot, feed_url, "ì—°í•©ë‰´ìŠ¤", category))
    
    # ë‰´ì‹œìŠ¤
    tasks.append(fetch_rss_feed(bot, RSS_FEEDS["ë‰´ì‹œìŠ¤"], "ë‰´ì‹œìŠ¤"))
    
    # í•œêµ­ê²½ì œ
    for feed_url, category in RSS_FEEDS["í•œêµ­ê²½ì œ"]:
        tasks.append(fetch_rss_feed(bot, feed_url, "í•œêµ­ê²½ì œ", category))
    
    # ë§¤ì¼ê²½ì œ
    tasks.append(fetch_rss_feed(bot, RSS_FEEDS["ë§¤ì¼ê²½ì œ"], "ë§¤ì¼ê²½ì œ"))
    
    # ì„œìš¸ê²½ì œ
    tasks.append(fetch_rss_feed(bot, RSS_FEEDS["ì„œìš¸ê²½ì œ"], "ì„œìš¸ê²½ì œ"))
    
    # í•œê²¨ë ˆ
    tasks.append(fetch_rss_feed(bot, RSS_FEEDS["í•œê²¨ë ˆ"], "í•œê²¨ë ˆ"))
    
    # ë³‘ë ¬ ì‹¤í–‰
    await asyncio.gather(*tasks)

async def fetch_naver_breaking(bot):
    """ë„¤ì´ë²„ ì†ë³´ (ë³´ì¡°)"""
    try:
        sections = [("100", "ì •ì¹˜"), ("101", "ê²½ì œ"), ("105", "IT")]
        
        for sid, name in sections:
            url = f"https://news.naver.com/section/{sid}"
            r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
            soup = BeautifulSoup(r.text, "html.parser")
            
            articles = soup.select("div.sa_text a.sa_text_title")[:5]
            
            for article in articles:
                title = article.get_text(strip=True)
                link = article.get("href", "")
                
                if not link:
                    continue
                
                title_hash = get_title_hash(title)
                if link in sent_news or title_hash in title_hashes:
                    continue
                
                # [ì†ë³´]ë§Œ
                if "[ì†ë³´]" in title:
                    await send_news(bot, "ì†ë³´", title, link, f"ë„¤ì´ë²„{name}")
                    sent_news.add(link)
                    title_hashes.add(title_hash)
                    save_data()
                    await asyncio.sleep(1)
                    
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„: {e}")

async def main():
    print("ğŸ”¥ ëˆ„ë ë´‡ PRO ë²„ì „!")
    print("ğŸ“° ë‹¤ì¤‘ ì–¸ë¡ ì‚¬ RSS ë³‘ë ¬ ëª¨ë‹ˆí„°ë§")
    print("ğŸ”” [ì†ë³´] + ğŸ¯ [ë‹¨ë…] + ğŸ“‹ [ê¸°íš] + ğŸ“Š ì—¬ë¡ ì¡°ì‚¬")
    print("ğŸ“¢ ì±„ë„: @gazzzza2025")
    print("â° 1ë¶„ë§ˆë‹¤ í™•ì¸ (10ë¶„ ì´ë‚´ ê¸°ì‚¬)\n")
    
    load_data()
    print(f"ğŸ“ ì €ì¥ëœ ë‰´ìŠ¤: {len(sent_news)}ê°œ")
    print(f"ğŸ“ ì €ì¥ëœ í•´ì‹œ: {len(title_hashes)}ê°œ\n")
    
    bot = Bot(BOT_TOKEN)
    
    try:
        now = datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')
        await bot.send_message(
            CHAT_ID,
            f"ğŸ”¥ <b>ëˆ„ë ë´‡ PRO</b>\n\n"
            f"â° {now}\n"
            f"ğŸ“° 7ê°œ ì–¸ë¡ ì‚¬ RSS ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\n"
            f"âœ… 10ë¶„ ì´ë‚´ ìµœì‹  ê¸°ì‚¬ë§Œ ì „ì†¡\n"
            f"âœ… ì œëª© í•´ì‹œ ì¤‘ë³µ ë°©ì§€",
            parse_mode="HTML"
        )
    except:
        pass
    
    while True:
        print(f"â° {datetime.now(KST).strftime('%H:%M:%S')} - RSS ë³‘ë ¬ í™•ì¸ ì¤‘...")
        
        # RSS ë³‘ë ¬ í¬ë¡¤ë§
        await fetch_all_rss(bot)
        
        # ë„¤ì´ë²„ ë³´ì¡°
        await fetch_naver_breaking(bot)
        
        print("â³ 1ë¶„ ëŒ€ê¸°...\n")
        await asyncio.sleep(60)

asyncio.run(main())
