"""VIP ì•Œë¦¼ë´‡ - ë¨¸ìŠ¤í¬/íŠ¸ëŸ¼í”„ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§"""
import os
import requests
import json
import logging
from bs4 import BeautifulSoup
from datetime import datetime

logger = logging.getLogger(__name__)

BOT_TOKEN = os.environ.get('SCHEDULE_BOT_TOKEN', '8734510853:AAHsqC3fQfC0K02-xrWEZgnh9ZDGUIi2P44')
CHAT_ID = '5132309076'
SENT_FILE = '/tmp/vip_sent.json'
HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}

TARGETS = [
    {'name': 'ì¼ë¡  ë¨¸ìŠ¤í¬', 'emoji': 'ğŸš€', 'queries': ['ì¼ë¡  ë¨¸ìŠ¤í¬', 'Elon Musk']},
    {'name': 'ë„ë„ë“œ íŠ¸ëŸ¼í”„', 'emoji': 'ğŸ‡ºğŸ‡¸', 'queries': ['íŠ¸ëŸ¼í”„', 'Donald Trump']},
]


def load_sent():
    try:
        with open(SENT_FILE, 'r') as f:
            return set(json.load(f))
    except:
        return set()


def save_sent(sent):
    try:
        with open(SENT_FILE, 'w') as f:
            json.dump(list(sent)[-300:], f)
    except:
        pass


def fetch_google_news(query, limit=10):
    """Google News RSS í•œêµ­ì–´íŒ í¬ë¡¤ë§"""
    articles = []
    try:
        url = f'https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR&ceid=KR:ko'
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')
        for item in soup.select('item')[:limit]:
            title = item.select_one('title').text.strip()
            link = item.select_one('link').text.strip() if item.select_one('link') else ''
            source = title.split(' - ')[-1].strip() if ' - ' in title else ''
            clean_title = title.rsplit(' - ', 1)[0].strip() if ' - ' in title else title
            # í•œê¸€ ê¸°ì‚¬ë§Œ í•„í„°ë§
            if not any("uac00" <= c <= "ud7a3" for c in clean_title):
                continue            
            pub_date = item.select_one('pubDate').text.strip() if item.select_one('pubDate') else ''
            articles.append({
                'title': clean_title,
                'source': source,
                'link': link,
                'pub_date': pub_date,
            })
    except Exception as e:
        logger.error(f"Google News í¬ë¡¤ë§ ì‹¤íŒ¨ [{query}]: {e}")
    return articles


def check_and_send():
    """ìƒˆ ë‰´ìŠ¤ í™•ì¸ í›„ ì „ì†¡"""
    sent = load_sent()
    new_count = 0

    for target in TARGETS:
        all_articles = []
        seen_titles = set()

        for query in target['queries']:
            articles = fetch_google_news(query, limit=10)
            for art in articles:
                if art['title'] not in seen_titles:
                    seen_titles.add(art['title'])
                    all_articles.append(art)

        for art in all_articles:
            if art['link'] in sent:
                continue
            sent.add(art['link'])

            message = (
                f"{target['emoji']} <b>{target['name']} ê´€ë ¨ ë‰´ìŠ¤</b>\n\n"
                f"ğŸ·ï¸ ì–¸ë¡ ì‚¬: {art['source']}\n"
                f"ğŸ“ ì œëª©: {art['title']}\n"
                f"ğŸ”— {art['link']}"
            )

            try:
                url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
                resp = requests.post(url, json={
                    'chat_id': CHAT_ID,
                    'text': message,
                    'parse_mode': 'HTML',
                    'disable_web_page_preview': True,
                }, timeout=10)
                if resp.status_code == 200:
                    print(f"âœ… {target['emoji']} {art['title'][:40]}")
                    new_count += 1
                else:
                    print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {resp.text}")
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")

    save_sent(sent)
    print(f"[VIPì•Œë¦¼ë´‡] ì™„ë£Œ â€” {new_count}ê°œ ì „ì†¡")


def run_vip_alert():
    check_and_send()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    check_and_send()
